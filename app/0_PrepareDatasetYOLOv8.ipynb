{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immport\n",
    "from xml.dom import minidom\n",
    "import bs4 as bs\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRM_INPUT_ROOT_PATH = 'datasets_raw'\n",
    "PRM_OUTPUT_ROOT_PATH = 'datasets'\n",
    "PRM_OUTPUT_BASE_PATH = 'labels/val'\n",
    "PRM_OUTPUT_MAIN_PATH = f'{PRM_OUTPUT_ROOT_PATH}/{PRM_OUTPUT_BASE_PATH}'\n",
    "PRM_WORDS_TO_IGNORE = ['Zone.Identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(list_of_words):\n",
    "  for filter_word in PRM_WORDS_TO_IGNORE:\n",
    "    list_of_words = [word for word in list_of_words if filter_word.upper() not in word.upper()]\n",
    "  return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertPascal2YOLOv8(filePath):\n",
    "\n",
    "    class_mapping = {\n",
    "        \"D00\": 0,\n",
    "        \"D10\": 1,\n",
    "        \"D20\": 2,\n",
    "        \"D40\": 3,\n",
    "        \"D01\": 4,\n",
    "        \"D11\": 5,\n",
    "        \"D43\": 6,\n",
    "        \"D44\": 7,\n",
    "        \"D50\": 8\n",
    "    }\n",
    "    \n",
    "    # reading content\n",
    "    file = open(filePath, \"r\")\n",
    "    contents = file.read()\n",
    "\n",
    "    # parsing\n",
    "    soup = bs.BeautifulSoup(contents, 'xml')\n",
    "    image_size = soup.find_all(\"size\")[0]\n",
    "    image_width = int(image_size.find_all(\"width\")[0].get_text())\n",
    "    image_height = int(image_size.find_all(\"height\")[0].get_text())\n",
    "    # print(\"w,h :\", image_width, image_height)\n",
    "\n",
    "    # Process Bounding Box\n",
    "    objects = soup.find_all(\"object\")\n",
    "\n",
    "    # Placeholder\n",
    "    bounding_box_list = []\n",
    "    class_list = []\n",
    "\n",
    "    for object in objects:\n",
    "        \n",
    "        # Object Class\n",
    "        _class = object.find_all(\"name\")[0].get_text()\n",
    "        \n",
    "        # Map the class to int number, if not defined > 10\n",
    "        _class = class_mapping.get(_class, 10)\n",
    "        class_list.append(_class)\n",
    "        \n",
    "        # Object Bounding Box\n",
    "        _xmin = float(object.find_all(\"xmin\")[0].get_text())\n",
    "        _ymin = float(object.find_all(\"ymin\")[0].get_text())\n",
    "        _xmax = float(object.find_all(\"xmax\")[0].get_text())\n",
    "        _ymax = float(object.find_all(\"ymax\")[0].get_text())\n",
    "\n",
    "        # Convert to YOLOv8 Annotation\n",
    "        # class x_center y_center width height\n",
    "        w = (_xmax - _xmin)\n",
    "        h = (_ymax - _ymin)\n",
    "        cx = _xmin + (w/2)\n",
    "        cy = _ymin + (h/2)\n",
    "\n",
    "        # Normalize\n",
    "        w = round((w / image_width), 4)\n",
    "        h = round((h / image_height), 4)\n",
    "        cx = round((cx / image_width), 4)\n",
    "        cy = round((cy / image_height), 4)\n",
    "\n",
    "        _bbox = [cx, cy, w, h]\n",
    "\n",
    "        # print(_class, cx, cy, w, h)\n",
    "\n",
    "        bounding_box_list.append(_bbox)\n",
    "\n",
    "    # Get the filename\n",
    "    outputFilename = os.path.split(filePath)[1]\n",
    "    outputFilename = outputFilename.replace(\".xml\", \".txt\")\n",
    "\n",
    "    # Output Path\n",
    "    outputDir = Path(filePath).parents[2]\n",
    "    outputDir = outputDir / \"labels\"\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "\n",
    "    # Final output path\n",
    "    outputPath = outputDir / outputFilename\n",
    "    # print(outputPath)\n",
    "\n",
    "    # Write to .txt file\n",
    "    with open(outputPath, 'w') as f:\n",
    "        for i in range(len(class_list)):\n",
    "\n",
    "            # Filter the class, drop unused class\n",
    "            # 0: D00 > Longitudinal Crack\n",
    "            # 1: D10 > Transverse Crack\n",
    "            # 2: D20 > Alligator Crack\n",
    "            # 3: D40 > Potholes\n",
    "            \n",
    "            if class_list[i] < 4:\n",
    "                anno = str(class_list[i]) + \" \" +  str(bounding_box_list[i][0]) + \" \" +  str(bounding_box_list[i][1]) + \" \" +  str(bounding_box_list[i][2]) + \" \" +  str(bounding_box_list[i][3]) + \"\\n\"\n",
    "                f.write(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('datasets_raw/datasets_raw/RDD2022_all_countries/United_States/United_States')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(ROOTDIR + CountryDir).parents[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10506/10506 [00:12<00:00, 863.97it/s]\n",
      "100%|██████████| 7706/7706 [00:06<00:00, 1226.14it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 1977/1977 [00:02<00:00, 848.57it/s]\n",
      "100%|██████████| 2829/2829 [00:01<00:00, 1529.23it/s]\n",
      "100%|██████████| 8161/8161 [00:07<00:00, 1144.02it/s]\n",
      "100%|██████████| 4805/4805 [00:04<00:00, 1034.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset Root Directory\n",
    "ROOTDIR = \"datasets_raw/\"\n",
    "\n",
    "# Base Directory\n",
    "CountryListDir = [\"RDD2022_all_countries/Japan/Japan/train/annotations/xmls\",\n",
    "                  \"RDD2022_all_countries/India/India/train/annotations/xmls\",\n",
    "                  \"RDD2022_all_countries/China_Drone/China_Drone/train/annotations/xmls\",\n",
    "                  \"RDD2022_all_countries/China_MotorBike/China_MotorBike/train/annotations/xmls\",\n",
    "                  \"RDD2022_all_countries/Czech/Czech/train/annotations/xmls\",\n",
    "                  \"RDD2022_all_countries/Norway/Norway/train/annotations/xmls\",\n",
    "                  \"RDD2022_all_countries/United_States/United_States/train/annotations/xmls\",\n",
    "]\n",
    "\n",
    "for CountryDir in CountryListDir:\n",
    "    \n",
    "    CountryDir = ROOTDIR + CountryDir\n",
    "    fileList = sorted(glob.glob(CountryDir + \"/*.xml\"))\n",
    "\n",
    "    # Processing all the annotation\n",
    "    for file in tqdm(fileList):\n",
    "        convertPascal2YOLOv8(file)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CopyDatasetSplit(baseDir):\n",
    "    \n",
    "    # Split the training data to train and validation data due to lack of annotation on test data\n",
    "    # Seed\n",
    "    random.seed(1337)\n",
    "    \n",
    "    # Output Directory\n",
    "    # !!! Change this to your clone folder\n",
    "    baseOutputDir = \"datasets/init_base\"\n",
    "    countryName = Path(baseDir).parents[0]\n",
    "    countryName = os.path.split(countryName)[1]\n",
    "    countryName = countryName.split('/')[0]\n",
    "\n",
    "    baseImageDir = baseDir + \"images/\"\n",
    "    baseAnnotDir = baseDir + \"labels/\"\n",
    "\n",
    "    image_list_all = sorted(glob.glob(baseImageDir + \"/*\"))\n",
    "    annot_list_all = sorted(glob.glob(baseAnnotDir + \"/*\"))\n",
    "\n",
    "    # Drop any images that doesnt have annotation (background)\n",
    "    # Or just leave it at some percentage of the dataset\n",
    "    backgroundImages_Percentage = 0.1\n",
    "    image_list = []\n",
    "    annot_list = []\n",
    "    \n",
    "    dataset_length_all = len(image_list_all)\n",
    "    max_background_image = int(dataset_length_all*backgroundImages_Percentage)\n",
    "    _counter = 0\n",
    "\n",
    "    for i in range(len(annot_list_all)):\n",
    "        \n",
    "        with open(annot_list_all[i]) as f:\n",
    "            _annot = f.read()\n",
    "\n",
    "            # Annotation not empty\n",
    "            if _annot:\n",
    "                image_list.append(image_list_all[i])\n",
    "                annot_list.append(annot_list_all[i])\n",
    "            elif _counter < max_background_image:\n",
    "                image_list.append(image_list_all[i])\n",
    "                annot_list.append(annot_list_all[i])\n",
    "                _counter = _counter + 1\n",
    "                \n",
    "    # Dataset length\n",
    "    dataset_length = len(image_list)\n",
    "    # print(dataset_length, len(annot_list))\n",
    "\n",
    "    split_ratio = 0.9\n",
    "    middle_point = round(split_ratio * dataset_length)\n",
    "\n",
    "    # Create random list number using seed\n",
    "    numberList = list(range(0, dataset_length))\n",
    "    random.shuffle(numberList)\n",
    "    trainNumberList = numberList[:middle_point]\n",
    "    validNumberList = numberList[middle_point:]\n",
    "    print(\"Training/Validation Samples :\", len(trainNumberList), len(validNumberList))\n",
    "\n",
    "    # Training images and labels\n",
    "    print(\"Copying training images and labels for\", countryName)\n",
    "    for i in tqdm(trainNumberList):\n",
    "\n",
    "        # Images\n",
    "        outputImagesDir = baseOutputDir + countryName + \"/images/train/\"\n",
    "        if not os.path.exists(outputImagesDir):\n",
    "            os.makedirs(outputImagesDir)\n",
    "\n",
    "        shutil.copy2(image_list[i], outputImagesDir)\n",
    "\n",
    "        # Annotations\n",
    "        outputAnnotDir = baseOutputDir + countryName + \"/labels/train/\"\n",
    "        if not os.path.exists(outputAnnotDir):\n",
    "            os.makedirs(outputAnnotDir)\n",
    "\n",
    "        shutil.copy2(annot_list[i], outputAnnotDir)\n",
    "        # print(outputImagesDir, outputAnnotDir)\n",
    "\n",
    "    # Validation images and labels\n",
    "    print(\"Copying validation images and labels for\", countryName)\n",
    "    for i in tqdm(validNumberList):\n",
    "\n",
    "        # Images\n",
    "        outputImagesDir = baseOutputDir + countryName + \"/images/val/\"\n",
    "        if not os.path.exists(outputImagesDir):\n",
    "            os.makedirs(outputImagesDir)\n",
    "\n",
    "        shutil.copy2(image_list[i], outputImagesDir)\n",
    "\n",
    "        # Annotations\n",
    "        outputAnnotDir = baseOutputDir + countryName + \"/labels/val/\"\n",
    "        if not os.path.exists(outputAnnotDir):\n",
    "            os.makedirs(outputAnnotDir)\n",
    "\n",
    "        shutil.copy2(annot_list[i], outputAnnotDir)\n",
    "        # print(outputImagesDir, outputAnnotDir)\n",
    "\n",
    "# baseDir = \"../dataset/RDD2022/RDD2022_all_countries/Japan/train/\"\n",
    "# CopyDatasetSplit(baseDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 9001 1000\n",
      "Copying training images and labels for Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9001/9001 [00:03<00:00, 2301.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for Japan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2436.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 4288 476\n",
      "Copying training images and labels for India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4288/4288 [00:01<00:00, 2538.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for India\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 476/476 [00:00<00:00, 2401.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 0 0\n",
      "Copying training images and labels for China_Drone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for China_Drone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 1473 164\n",
      "Copying training images and labels for Czech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1473/1473 [00:00<00:00, 8820.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for Czech\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 6114.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 4091 455\n",
      "Copying training images and labels for Norway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4091/4091 [00:04<00:00, 858.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for Norway\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455/455 [00:00<00:00, 836.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 4324 481\n",
      "Copying training images and labels for United_States\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4324/4324 [00:00<00:00, 4397.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for United_States\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 481/481 [00:00<00:00, 4853.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Validation Samples : 1779 198\n",
      "Copying training images and labels for China_MotorBike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1779/1779 [00:00<00:00, 1868.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation images and labels for China_MotorBike\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 2014.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Base Directory\n",
    "ROOTDIR = \"datasets_raw/\"\n",
    "\n",
    "# Use only japan india\n",
    "CountryListDir = [\"RDD2022_all_countries/Japan/Japan/train/\",\n",
    "                  \"RDD2022_all_countries/India/India/train/\",\n",
    "                  \"RDD2022_all_countries/China_Drone/China_Drone/train/\",\n",
    "                  \"RDD2022_all_countries/Czech/Czech/train/\",\n",
    "                  \"RDD2022_all_countries/Norway/Norway/train/\",\n",
    "                  \"RDD2022_all_countries/United_States/United_States/train/\",\n",
    "                  \"RDD2022_all_countries/China_MotorBike/China_MotorBike/train/\",\n",
    "]\n",
    "\n",
    "for CountryDir in CountryListDir:\n",
    "    CountryDir = ROOTDIR + CountryDir\n",
    "    CopyDatasetSplit(CountryDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: tree: not found\n"
     ]
    }
   ],
   "source": [
    "!tree ./ -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
